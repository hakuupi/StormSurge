{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e2ef1ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed03be71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def bedtopo(L,*args):  #bed to topography L- length of glacier\n",
    "    b0 = args[0]   #b0: glacier start point\n",
    "    bx = args[1]   #bx: glacier slope\n",
    "    sillmin = args[2]   #start of reverse slope\n",
    "    sillmax = args[3]   #end of reverse slope\n",
    "    sillslope = args[4]   #slope of reverse slope\n",
    "    \n",
    "    if L < sillmin: #if glacier is shorter than reverse slope\n",
    "        b = b0 + bx*L\n",
    "    elif L < sillmax: #if glacier terminates on reverse slope\n",
    "        b = b0 + bx*sillmin + sillslope*(L-sillmin)\n",
    "    else: #if glacier terminates after reverse slope\n",
    "        b = b0 + bx*sillmin + sillslope*(sillmax-sillmin) + bx*(L-sillmax)\n",
    "        \n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e83cb849",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def TwoStage_timedep(state,*args): #Robel et al. 2018 two-stage model\n",
    "    \n",
    "    Hnd, Lnd = state #Unpack the state vector, Hnd n-dim thickness\n",
    "    #Lnd n-dim length\n",
    "    f = np.zeros(2) #Derivatives\n",
    "    \n",
    "    smb0 = args[0] #P (precipitation) at time 0\n",
    "    smb1 = args[1] #P (precipitation) at time 1\n",
    "    smbf = args[2] #P (precipitation) at time f\n",
    "    gamma = args[3] #Q coefficient\n",
    "    omega = args[4] #Q_g coefficient\n",
    "    time = args[5]\n",
    "    t1 = args[6]\n",
    "    tfinal = args[7]\n",
    "#    b0 = args[3]\n",
    "#    bx = args[4]\n",
    "#    sillmin = args[5]\n",
    "#    sillmax = args[6]\n",
    "#    sillslope = args[7]\n",
    "\n",
    "    b0 = 0   #glacier start point\n",
    "    bx = args[11]   #glacier slope\n",
    "    sillmin = args[8]*1e3   #reverse slope start point\n",
    "    sillmax = args[9]*1e3   #reverse slope ends point\n",
    "    sillslope = args[10]    #reverse slope slope\n",
    "    \n",
    "    rhow = 1028   #density of water\n",
    "    rhoi = 917   #denisty of ice\n",
    "    n = 3   \n",
    "    beta = 3 \n",
    "    Lscale = 100e3   #scale for length, it is on the order of 10E4 or 1E5\n",
    "    Hscale = 1000   #scale for thickness, it is on the order of 10E2 or 1E3\n",
    "    \n",
    "    H = Hnd*Hscale #icethickness\n",
    "    L = Lnd*Lscale #length of glacier\n",
    "    \n",
    "    if time < t1:\n",
    "        smb = smb0 + (smb1-smb0)*time/t1\n",
    "    else:\n",
    "        smb = smb1 + (smbf-smb1)*time/(tfinal-t1) \n",
    "    #see below for write out of the functions in mathematical notation\n",
    "    hg = -(rhow/rhoi)*bedtopo(L,b0,bx,sillmin,sillmax,sillslope)   # #1\n",
    "    Q = gamma * (H**(2*n + 1))/(L**n)   # #2\n",
    "    Qg = omega * (hg**beta)   # #3\n",
    "    \n",
    "    f[0] = (smb - (Qg/L) - (H/(hg*L))*(Q-Qg))/Hscale   # #4\n",
    "    f[1] = (Q-Qg)/hg/Lscale   # #5\n",
    "    return f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a16e6f1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def RK4(rhs,state,dt,*args): #ask for explanation\n",
    "    \n",
    "    k1 = rhs(state,*args)\n",
    "    k2 = rhs(state+k1*dt/2,*args)\n",
    "    k3 = rhs(state+k2*dt/2,*args)\n",
    "    k4 = rhs(state+k3*dt,*args)\n",
    "\n",
    "    new_state = state + (dt/6)*(k1+2*k2+2*k3+k4)\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5ec9730",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def EnKF(ubi,w,ObsOp,JObsOp,R,B,N):\n",
    "    \n",
    "    # The analysis step for the (stochastic) ensemble Kalman filter \n",
    "    # with virtual observations\n",
    "\n",
    "    n,N = ubi.shape # n is the state dimension and N is the size of ensemble\n",
    "    m = w.shape[0] # m is the size of measurement vector\n",
    "\n",
    "    # compute the mean of forecast ensemble\n",
    "    ub = np.mean(ubi,1)\n",
    "    # compute Jacobian of observation operator at ub\n",
    "    Dh = JObsOp(w)\n",
    "    # compute Kalman gain\n",
    "    D = Dh@B@Dh.T + R #@ is matrix multiplication\n",
    "    K = B @ Dh.T @ np.linalg.inv(D)\n",
    "    Q=np.cov(ubi)\n",
    "    ubi=ubi+np.random.multivariate_normal(np.zeros(n),Q,size=N).T\n",
    "\n",
    "    wi = np.zeros([m,N])\n",
    "    uai = np.zeros([n,N])\n",
    "    for i in range(N):\n",
    "        # create virtual observations\n",
    "        wi[:,i] = w + np.random.multivariate_normal(np.zeros(m), R)\n",
    "        # compute analysis ensemble\n",
    "        uai[:,i] = ubi[:,i] + K @ (wi[:,i]-ObsOp(ubi[:,i]))\n",
    "        \n",
    "    # compute the mean of analysis ensemble\n",
    "    ua = np.mean(uai,1)    \n",
    "    # compute analysis error covariance matrix\n",
    "    P = (1/(N-1)) * (uai - ua.reshape(-1,1)) @ (uai - ua.reshape(-1,1)).T\n",
    "    ve=np.linalg.norm(P)\n",
    "    return uai, P, ve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aff2b5cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Observation operators\n",
    "def h(u):   #identity oberservation operator?\n",
    "    w = u\n",
    "    return w\n",
    "\n",
    "def Dh(u):   #identity operator for matrix of size Dh\n",
    "    n = len(u)\n",
    "    D = np.eye(n)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf409e58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "err = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3e2a189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load in data and turn it into the uTrue\n",
    "u=pd.read_csv(\"/Users/loganknudsen/Documents/GitHub/StormSurge/Experiment_Data/data_single_cases/truth_simulation/smbt11950smbtf2300smb00.3smb10.15smbf0.0sillmin415sillmax425sillslope0.01hnd2.18lnd4.44bx-0.001.csv\")\n",
    "b=np.zeros((len(u),2))\n",
    "b[:,0]=u[\"H\"]\n",
    "b[:,1]=u[\"L\"]\n",
    "uTrue=b.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa1b011d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EMILY EDIT THIS PART\n",
    "\n",
    "nd = 2 #degrees of freedom (number of prognositic equations in model)\n",
    "# parameters\n",
    "smb0 = 0.35\n",
    "smb1 = 0.15\n",
    "smbf = 0.0\n",
    "t1 = 1950\n",
    "gamma = 0.05\n",
    "omega = 8e-8\n",
    "#b0 = 0\n",
    "bx = -0.001\n",
    "sillmin = 415e3\n",
    "sillmax = 425e3\n",
    "sillslope = 0.008\n",
    "\n",
    "dt = 0.25\n",
    "tm = 2300\n",
    "nt = int(tm/dt)\n",
    "#all possible time values\n",
    "t = np.linspace(0,tm,nt+1)\n",
    "u0True = np.array([2.18,4.44]) # True initial conditions\n",
    "# first is H, second is L\n",
    "#np.random.seed(seed=1)\n",
    "sig_m_acc= 0.07  # standard deviation for accurate measurement noise\n",
    "sig_m_inacc= 0.1  # standard deviation for inaccurate measurement noise\n",
    "#chooses dates for \"measurements\" and create list of them\n",
    "ind_m_inacc = np.linspace(0*4,1900*4, 100).astype(int) #np.array([1000,1900])\n",
    "ind_m_acc = np.linspace(1950*4,2300*4, 350).astype(int) #EDIT HEREREREREEREREREREREREREREREREREERERERERERERERERER\n",
    "ind_m = np.concatenate((ind_m_inacc,ind_m_acc))\n",
    "#pullls dates from all possible time values\n",
    "t_m = t[ind_m]\n",
    "nt_m = np.size(ind_m)\n",
    "#creates list of standard deviation to use\n",
    "sig_m =  np.zeros(nt+1)\n",
    "sig_m[ind_m_inacc] = sig_m_inacc\n",
    "sig_m[ind_m_acc] = sig_m_acc\n",
    "\n",
    "#time integration\n",
    "# uTrue = np.zeros([nd,nt+1])\n",
    "# uTrue[:,0] = u0True\n",
    "km = 0 #loop variable i\n",
    "w = np.zeros([nd,nt_m]) # w = will hold 'measurements'\n",
    "for k in range(nt):\n",
    "    time = k*dt\n",
    "    if (km<nt_m) and (k+1==ind_m[km]): # if before measurement step: don't run. If after measurement step: run.\n",
    "        w[:,km] = h(uTrue[:,k+1]) + np.random.normal(0,sig_m[k+1],[nd,]) # generates list of measurements from true values with normal randomness added\n",
    "        km = km+1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "8370b881",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9201"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62f56839",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#save all \"obs\"\n",
    "ind_m_all = ind_m\n",
    "nt_m_all = nt_m\n",
    "w_all = w\n",
    "t_m_all = t_m\n",
    "sig_m_all = sig_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acdc0c7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "u0b = np.array([2.3, 4.6]) #\"wrong\" initial conditions (initial condition of simulation)\n",
    "#omega = 0.9*8e-8 #\"wrong\" parameter\n",
    "\n",
    "sig_b= 0.1\n",
    "B = sig_b**2*np.eye(nd) #placeholder until first filter update\n",
    "Q = 0.1*np.eye(nd)\n",
    "\n",
    "#time integration\n",
    "ub = np.zeros([nd,nt+1])\n",
    "ub[:,0] = u0b\n",
    "ua = np.zeros([nd,nt+1])\n",
    "ua[:,0] = u0b\n",
    "\n",
    "n = nd #state dimension\n",
    "m = nd #measurement dimension\n",
    "\n",
    "# ensemble size \n",
    "N = 10\n",
    "#initialize ensemble\n",
    "uai = np.zeros([nd,N])\n",
    "uae = np.zeros([nd,N,nt+1])\n",
    "for i in range(N):\n",
    "    uai[:,i] = u0b + np.random.multivariate_normal(np.zeros(n), B)\n",
    "Q=np.cov(uai)\n",
    "## which obs to assimilate\n",
    "w = w_all\n",
    "ind_m = ind_m_all\n",
    "nt_m = nt_m_all\n",
    "t_m = t_m_all\n",
    "sig_m =  sig_m_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16a193ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n0/rsxpkn6j2xv8hrpk19m6_r6m0000gn/T/ipykernel_10726/3805134311.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mavgStdErrorH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdErrorH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdErrorH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0mavgStdErrorL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdErrorL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdErrorL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mavgImpactH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpactH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpactH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "km = 0\n",
    "p = []\n",
    "stdErrorH = [] #Standard Error (Analysis-Truth)^2\n",
    "stdErrorL = [] \n",
    "impactH = [] #(Background - Analysis)^2 \n",
    "impactL = []\n",
    "ogErrorH = [] #(Background - Truth)^2 \n",
    "ogErrorL = []\n",
    "pctErrorH = [] # Percent Error absolute value ((Analysis-Truth)/Truth)\n",
    "pctErrorL = []\n",
    "ogPctErrorH = []# absolute value(Background - Truth)/Truth\n",
    "ogPctErrorL = []\n",
    "\n",
    "for k in range(nt):\n",
    "    time = k*dt\n",
    "    # Forecast Step\n",
    "    #background trajectory [without correction]\n",
    "    ub[:,k+1] = RK4(TwoStage_timedep,ub[:,k],dt,smb0,smb1,smbf,gamma,omega,time,t1,tm,sillmin,sillmax,sillslope,bx)\n",
    "    #EnKF trajectory [with correction at observation times]\n",
    "    for i in range(N): # forecast ensemble\n",
    "        uai[:,i] = RK4(TwoStage_timedep,uai[:,i],dt,smb0,smb1,smbf,gamma,omega,time,t1,tm,sillmin,sillmax,sillslope,bx)\n",
    "        uae[:,i,k+1] = uai[:,i]\n",
    "\n",
    "    # compute the mean of forecast ensemble\n",
    "    ua[:,k+1] = np.mean(uai,1)\n",
    "    # compute forecast error covariance matrix\n",
    "    B = (1/(N-1)) * (uai - ua[:,k+1].reshape(-1,1)) @ (uai - ua[:,k+1].reshape(-1,1)).T\n",
    "    \n",
    "    if (km<nt_m) and (k+1==ind_m[km]):\n",
    "        R = sig_m[k+1]**2*np.eye(nd) #covariance matrix for measurement noise\n",
    "\n",
    "        # Analysis Step\n",
    "        uai,B,ve = EnKF(uai,w[:,km],h,Dh,R,B,N)\n",
    "        p.append(ve.tolist())\n",
    "        # compute the mean of analysis ensemble\n",
    "        ua[:,k+1] = np.mean(uai,1)\n",
    "        km = km+1\n",
    "    if ind_m_inacc.size == 0:\n",
    "        if time > 1950:\n",
    "            stdErrorH.append((uTrue[0,k]-ua[0,k])**2)  \n",
    "            stdErrorL.append((uTrue[1,k]-ua[1,k])**2)\n",
    "            pctErrorH.append(abs(uTrue[0,k]-ua[0,k])/uTrue[0,k])\n",
    "            pctErrorL.append(abs(uTrue[1,k]-ua[1,k])/uTrue[1,k]) \n",
    "            impactH.append((ub[0,k]-ua[0,k])**2)\n",
    "            impactL.append((ub[1,k]-ua[1,k])**2)\n",
    "            ogErrorH.append((uTrue[0,k]-ub[0,k])**2)  \n",
    "            ogErrorL.append((uTrue[1,k]-ub[1,k])**2)\n",
    "            ogPctErrorH.append(abs(uTrue[0,k]-ub[0,k])/uTrue[0,k])  \n",
    "            ogPctErrorL.append(abs(uTrue[1,k]-ub[1,k])/uTrue[1,k])\n",
    "\n",
    "    elif ind_m_acc.size ==0:\n",
    "        if time < 1900:\n",
    "            stdErrorH.append((uTrue[0,k]-ua[0,k])**2)  \n",
    "            stdErrorL.append((uTrue[1,k]-ua[1,k])**2)\n",
    "            pctErrorH.append(abs(uTrue[0,k]-ua[0,k])/uTrue[0,k])\n",
    "            pctErrorL.append(abs(uTrue[1,k]-ua[1,k])/uTrue[1,k])\n",
    "            impactH.append((ub[0,k]-ua[0,k])**2)\n",
    "            impactL.append((ub[1,k]-ua[1,k])**2)\n",
    "            ogErrorH.append((uTrue[0,k]-ub[0,k])**2)  \n",
    "            ogErrorL.append((uTrue[1,k]-ub[1,k])**2)\n",
    "            ogPctErrorH.append(abs(uTrue[0,k]-ub[0,k])/uTrue[0,k])  \n",
    "            ogPctErrorL.append(abs(uTrue[1,k]-ub[1,k])/uTrue[1,k])\n",
    "  \n",
    "\n",
    "\n",
    "avgStdErrorH = sum(stdErrorH)/len(stdErrorH)\n",
    "avgStdErrorL = sum(stdErrorL)/len(stdErrorL)\n",
    "avgImpactH = sum(impactH)/len(impactH)\n",
    "avgImpactL = sum(impactL)/len(impactL)\n",
    "avgOgErrorH = sum(ogErrorH)/len(ogErrorH)\n",
    "avgOgErrorL = sum(ogErrorL)/len(ogErrorL)\n",
    "avgOgPctErrorH = sum(ogPctErrorH)/len(ogPctErrorH)\n",
    "avgOgPctErrorL = sum(ogPctErrorL)/len(ogPctErrorL)\n",
    "avgPctErrorH = sum(pctErrorH)/len(pctErrorH)\n",
    "avgPctErrorL = sum(pctErrorL)/len(pctErrorL)\n",
    "\n",
    "#p is list of values for black line (covariance norms)\n",
    "len(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e65dafa6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations before 1900:  100 , assimilated every  19.0  years\n",
      "Number of observations after 1950:  350 , assimilated every  1  years\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'avgStdErrorH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n0/rsxpkn6j2xv8hrpk19m6_r6m0000gn/T/ipykernel_10726/2076205951.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Standard Error (Analysis-Truth)^2 for H: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavgStdErrorH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\", and L: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavgStdErrorL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Percent Error (Analysis-Truth)/Truth for H: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavgPctErrorH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\", and L: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavgPctErrorL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Background - Analysis)^2 for H: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavgImpactH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\", and L: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavgImpactL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'avgStdErrorH' is not defined"
     ]
    }
   ],
   "source": [
    "if ind_m_inacc.size != 0:\n",
    "    print(\"Number of observations before 1900: \", ind_m_inacc.size, \", assimilated every \", 1900/ind_m_inacc.size, \" years\")\n",
    "else: \n",
    "    print(\"Number of observations before 1950: \", ind_m_inacc.size)\n",
    "\n",
    "\n",
    "if ind_m_acc.size != 0:\n",
    "    if 350/ind_m_acc.size >= 1:\n",
    "        print(\"Number of observations after 1950: \", ind_m_acc.size, \", assimilated every \", round(350/ind_m_acc.size), \" years\")\n",
    "    else:\n",
    "        print(\"Number of observations after 1950: \", ind_m_acc.size, \", assimilated every \", 350/ind_m_acc.size, \" years\")\n",
    "else: \n",
    "    print(\"Number of observations after 1950: \", ind_m_acc.size)\n",
    "\n",
    "\n",
    "print(\"Standard Error (Analysis-Truth)^2 for H: \", round(avgStdErrorH,7), \", and L: \",round(avgStdErrorL,7))\n",
    "print(\"Percent Error (Analysis-Truth)/Truth for H: \", round(avgPctErrorH,7), \", and L: \",round(avgPctErrorL,7))\n",
    "print(\"Background - Analysis)^2 for H: \", round(avgImpactH,7), \", and L: \",round(avgImpactL,7))\n",
    "print(\"Original Error (Truth - Background)^2 for H: \", round(avgOgErrorH,7), \", and L: \",round(avgOgErrorL,7))\n",
    "print(\"Original Percent Error (Truth - Background)/Truth for H: \", round(avgOgPctErrorH,7), \", and L: \",round(avgOgPctErrorL,7))\n",
    "print(\" - note: all rounded to 7 significant digits\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=nd,ncols=1, figsize=(10,8))\n",
    "ax = ax.flat\n",
    "\n",
    "\n",
    "for k in range(nd):\n",
    "    ax[k].plot(t,uTrue[k,:], label='Truth (hidden)', linewidth = 4)\n",
    "    ax[k].plot(t,ub[k,:], ':', label='Background', linewidth = 3)\n",
    "    '''ax[k].plot(t[ind_m],w[k,:], 'o', fillstyle='none', \\\n",
    "               label='Observations', markersize = 8, markeredgewidth = 2, color = 'g')''' # observations\n",
    "    ax[k].set_xlabel('t',fontsize=14)\n",
    "    #ax[k].axvspan(0, tm_m, color='y', alpha=0.4, lw=0)\n",
    "    \"\"\"for j in range(N):\n",
    "        ax[k].plot(t[1:],uae[k,j,1:], '-', linewidth = 1, color='r') #the ensembles \"\"\"\n",
    "    ax[k].plot(t,ua[k,:], '-', label='Analysis', linewidth = 1, color = 'k')\n",
    "\n",
    "ax[0].legend(loc=\"center\", bbox_to_anchor=(0.5,1.25),ncol =4,fontsize=15)\n",
    "\n",
    "ax[0].set_ylabel('H(t)', labelpad=5, fontsize=14)\n",
    "ax[1].set_ylabel('L(t)', labelpad=5, fontsize=14)\n",
    "#ax[2].set_ylabel('z(t)')\n",
    "fig.subplots_adjust(hspace=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da895253",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainty in Assimilation\n",
      "Number of observations before 1950:  100\n",
      "Number of observations after 1950:  350\n",
      "Mean of covariance norms:  nan\n",
      "Variance of covariance norms:  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/loganknudsen/opt/anaconda3/envs/geo/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/loganknudsen/opt/anaconda3/envs/geo/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/loganknudsen/opt/anaconda3/envs/geo/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3723: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/Users/loganknudsen/opt/anaconda3/envs/geo/lib/python3.8/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/Users/loganknudsen/opt/anaconda3/envs/geo/lib/python3.8/site-packages/numpy/core/_methods.py:254: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation minimum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n0/rsxpkn6j2xv8hrpk19m6_r6m0000gn/T/ipykernel_10726/3403541730.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mean of covariance norms: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Variance of covariance norms: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"minimum of covariance norms: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"maximum of covariance norms: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" - note: all rounded to 7 significant digits\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind_m\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamin\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamin\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2877\u001b[0m     \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2878\u001b[0m     \"\"\"\n\u001b[0;32m-> 2879\u001b[0;31m     return _wrapreduction(a, np.minimum, 'min', axis, None, out,\n\u001b[0m\u001b[1;32m   2880\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[1;32m   2881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation minimum which has no identity"
     ]
    }
   ],
   "source": [
    "print(\"Uncertainty in Assimilation\")\n",
    "print(\"Number of observations before 1950: \", ind_m_inacc.size)\n",
    "print(\"Number of observations after 1950: \", ind_m_acc.size)\n",
    "\n",
    "print(\"Mean of covariance norms: \", round(np.mean(p),7))\n",
    "print(\"Variance of covariance norms: \", round(np.var(p),7))\n",
    "print(\"minimum of covariance norms: \", round(np.min(p),7))\n",
    "print(\"maximum of covariance norms: \", round(np.max(p),7), \" - note: all rounded to 7 significant digits\")\n",
    "plt.scatter(ind_m[:]/4,p[:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "c5abda86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot number of observations against mean squared error\n",
    "oldDataObs = [200,100,50,25,10]\n",
    "oldDataHErrors = []\n",
    "oldDataLErrors = []\n",
    "newDataObs = [1400,700,350,175,88,44,22]\n",
    "newDataHErrors = []\n",
    "newDataLErrors = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "95d7987af6d225818fa272370bf1498d4d3a2d2b2e8eafb187a2c72de2276827"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
